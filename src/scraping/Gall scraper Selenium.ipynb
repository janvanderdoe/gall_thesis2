{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium.webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "from time import sleep\n",
    "import json\n",
    "import csv\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = selenium.webdriver.Chrome(executable_path=\"C:/Users/janva/Downloads/chromedriver.exe\")\n",
    "driver.get(\"https://www.gall.nl/grolsch-lentebok-30cl-992739.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = driver.page_source.encode('utf-8')\n",
    "soup = BeautifulSoup(res, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soup(url):\n",
    "    #driver = selenium.webdriver.Chrome(executable_path=\"C:/Users/janva/Downloads/chromedriver.exe\")\n",
    "    #driver.get(url)\n",
    "    #res = driver.page_source.encode('utf-8')\n",
    "    #soup = BeautifulSoup(res, \"html.parser\")\n",
    "    r = requests.get(url, headers={\"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.0.0 Safari/537.36\"})\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = get_soup(\"https://www.gall.nl/grolsch-lentebok-30cl-992739.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(\"https://www.gall.nl/grolsch-lentebok-30cl-992739.html\", headers={\"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.0.0 Safari/537.36\"})\n",
    "soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = get_soup(\"https://www.gall.nl/lawsons-dry-hills-riesling-wit-75cl-121304.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"url\" in json.loads(str(soup.find('script', {\"type\":\"application/ld+json\"}).contents[0].replace('\\n', '')))[0]:\n",
    "    print(\"yes\")\n",
    "else:\n",
    "    print('no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "reviews = json.loads(str(soup.find('script', {\"type\":\"application/ld+json\"}).contents[0].replace('\\n', '')))[0]\n",
    "if \"review\" in reviews:\n",
    "    for review in reviews['review']:\n",
    "        info = {'link' : \"asdf\",\n",
    "                \"rating\" : review['reviewRating']['ratingValue'],\n",
    "                \"date\" : review['datePublished'],\n",
    "                \"review\" : review['description']}\n",
    "        try:\n",
    "            info['type'] = soup.find('span', class_=\"pdp-info_profile\").text.strip()\n",
    "        except:\n",
    "            info['type'] = None\n",
    "        try:\n",
    "            info['rating_average'] = float(soup.find('span', class_ =\"rating_label\").text)\n",
    "        except:\n",
    "            info['rating_average'] = None\n",
    "        try:\n",
    "            info['count'] = int(soup.find('span', class_ =\"rating_label\")['data-count'])\n",
    "        except:\n",
    "            info['count'] = None\n",
    "        try:\n",
    "            info['title'] = soup.find('h1', class_ =\"pdp-info_name\").text\n",
    "        except:\n",
    "            info['title'] = None\n",
    "        try:\n",
    "            info['country'] = soup.find('span', class_ = re.compile(\"flag-round u-flag-large u-hidden\"))['title']\n",
    "        except:\n",
    "            info['country'] = None\n",
    "        try:\n",
    "            info['size'] = soup.find('p', class_ =\"pdp-info_desc\").find_all('i')[0].text\n",
    "        except:\n",
    "            info['size'] = None\n",
    "        try:\n",
    "            info['percentage'] = soup.find('p', class_ =\"pdp-info_desc\").find_all('i')[1].text\n",
    "        except:\n",
    "            info['percentage'] = None\n",
    "        try:\n",
    "            info['year'] = soup.find('p', class_ =\"pdp-info_desc\").find_all('span')[2].text\n",
    "        except:\n",
    "            info['year'] = None\n",
    "        try:\n",
    "            info['badge'] = soup.find('p', class_ =\"pdp-info_badges\").text.strip()\n",
    "        except:\n",
    "            info['badge'] = None\n",
    "        try:\n",
    "            info['price'] = soup.find('strong', class_ = \"price-v2 pdp-info_price base-price\")['aria-label'].replace('€ ', '')\n",
    "        except:\n",
    "            info['price'] = None\n",
    "        try:\n",
    "            info['main_description'] = soup.find('div', class_ =\"pdp-info_short s-rich-text\").text.strip()\n",
    "        except:\n",
    "            info['main_description'] = None\n",
    "        try:\n",
    "            usps = []\n",
    "            for item in soup.find('ul', class_ = \"m-usp-bar\").find_all('div', class_=\"usp__text\"):\n",
    "                usps.append(item.text.strip())\n",
    "            info['usps'] = usps\n",
    "        except:\n",
    "            info['usps'] = None\n",
    "\n",
    "        try:\n",
    "            info['secondary_description'] = soup.find('div', class_ = \"pdp-details-description s-rich-text\").text.strip()\n",
    "        except:\n",
    "            info['secondary_description'] = None\n",
    "        details = [e.text for e in soup.find_all('span', class_ =\"pdp-details__item-label\")]\n",
    "        try:\n",
    "            pos = details.index(\"Smaak\")\n",
    "            info['taste'] = soup.find_all('span', class_ =\"pdp-details__item-label-secondary\")[pos].text.replace('\\n', '')\n",
    "        except:\n",
    "            info['taste'] = None\n",
    "        try:\n",
    "            pos = details.index(\"Lekker bij\")\n",
    "            info['complementaries'] = soup.find_all('span', class_ =\"pdp-details__item-label-secondary\")[pos].text.replace('\\n', '')\n",
    "        except:\n",
    "            info['complementaries'] = None\n",
    "        try:\n",
    "            pos = details.index(\"Druivensoort\")\n",
    "            info['grape'] = soup.find_all('span', class_ =\"pdp-details__item-label-secondary\")[pos].text.replace('\\n', '')\n",
    "        except:\n",
    "            info['grape'] = None\n",
    "        try:\n",
    "            pos = details.index(\"Etiketinformatie\")\n",
    "            info['label'] = soup.find_all('span', class_ =\"pdp-details__item-label-secondary\")[pos].text.replace('\\n', '')\n",
    "        except:\n",
    "            info['label'] = None\n",
    "        try:\n",
    "            pos = details.index(\"Prijswinnend\")\n",
    "            info['certificate'] = soup.find_all('span', class_ =\"pdp-details__item-label-secondary\")[pos].text.replace('\\n', '')\n",
    "        except:\n",
    "            info['certificate'] = None\n",
    "\n",
    "        try:\n",
    "            info['producer_description'] = soup.find('div', class_=\"background-story__description s-rich-text read-more__text\").text.strip()\n",
    "        except:\n",
    "            info['producer_description'] = None\n",
    "        try:\n",
    "            info['producer'] = soup.find('span', class_=\"background-story__name\").text\n",
    "        except:\n",
    "            info['producer'] = None\n",
    "        data.append(info)\n",
    "else:\n",
    "    pass\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find('p', class_ =\"pdp-info_desc\").find_all('span')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"C:/Users/janva/Documents/Git projects/gall_thesis2/gen/input/no_reviews.csv\", \"w\", encoding = 'utf-8-sig', newline = '') as csv_file:\n",
    "    writer = csv.writer(csv_file, delimiter = \";\")\n",
    "    writer.writerow(['link'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_product_info(product_url, data):\n",
    "    soup = get_soup(product_url)\n",
    "    reviews = json.loads(str(soup.find('script', {\"type\":\"application/ld+json\"}).contents[0].replace('\\n', '')))[0]\n",
    "    #Check if there are reviews\n",
    "    if \"review\" in reviews:\n",
    "        for review in reviews['review']:\n",
    "            info = {'link' : product_url,\n",
    "                    \"rating\" : review['reviewRating']['ratingValue'],\n",
    "                    \"date\" : review['datePublished'],\n",
    "                    \"review\" : review['description']}\n",
    "            try:\n",
    "                info['type'] = soup.find('span', class_=\"pdp-info_profile\").text.strip()\n",
    "            except:\n",
    "                info['type'] = None\n",
    "            try:\n",
    "                info['rating_average'] = float(soup.find('span', class_ =\"rating_label\").text)\n",
    "            except:\n",
    "                info['rating_average'] = None\n",
    "            try:\n",
    "                info['count'] = int(soup.find('span', class_ =\"rating_label\")['data-count'])\n",
    "            except:\n",
    "                info['count'] = None\n",
    "            try:\n",
    "                info['title'] = soup.find('h1', class_ =\"pdp-info_name\").text\n",
    "            except:\n",
    "                info['title'] = None\n",
    "            try:\n",
    "                info['country'] = soup.find('span', class_ = re.compile(\"flag-round u-flag-large u-hidden\"))['title']\n",
    "            except:\n",
    "                info['country'] = None\n",
    "            try:\n",
    "                info['size'] = soup.find('p', class_ =\"pdp-info_desc\").find_all('i')[0].text\n",
    "            except:\n",
    "                info['size'] = None\n",
    "            try:\n",
    "                info['percentage'] = soup.find('p', class_ =\"pdp-info_desc\").find_all('i')[1].text\n",
    "            except:\n",
    "                info['percentage'] = None\n",
    "            try:\n",
    "                info['year'] = soup.find('p', class_ =\"pdp-info_desc\").find_all('span')[2].text\n",
    "            except:\n",
    "                info['year'] = None\n",
    "            try:\n",
    "                info['badge'] = soup.find('p', class_ =\"pdp-info_badges\").text.strip()\n",
    "            except:\n",
    "                info['badge'] = None\n",
    "            try:\n",
    "                info['price'] = soup.find('strong', class_ = \"price-v2 pdp-info_price base-price\")['aria-label'].replace('€ ', '')\n",
    "            except:\n",
    "                info['price'] = None\n",
    "            try:\n",
    "                info['main_description'] = soup.find('div', class_ =\"pdp-info_short s-rich-text\").text.strip()\n",
    "            except:\n",
    "                info['main_description'] = None\n",
    "            try:\n",
    "                usps = []\n",
    "                for item in soup.find('ul', class_ = \"m-usp-bar\").find_all('div', class_=\"usp__text\"):\n",
    "                    usps.append(item.text.strip())\n",
    "                info['usps'] = usps\n",
    "            except:\n",
    "                info['usps'] = None\n",
    "\n",
    "            try:\n",
    "                info['secondary_description'] = soup.find('div', class_ = \"pdp-details-description s-rich-text\").text.strip()\n",
    "            except:\n",
    "                info['secondary_description'] = None\n",
    "            details = [e.text for e in soup.find_all('span', class_ =\"pdp-details__item-label\")]\n",
    "            try:\n",
    "                pos = details.index(\"Smaak\")\n",
    "                info['taste'] = soup.find_all('span', class_ =\"pdp-details__item-label-secondary\")[pos].text.replace('\\n', '')\n",
    "            except:\n",
    "                info['taste'] = None\n",
    "            try:\n",
    "                pos = details.index(\"Lekker bij\")\n",
    "                info['complementaries'] = soup.find_all('span', class_ =\"pdp-details__item-label-secondary\")[pos].text.replace('\\n', '')\n",
    "            except:\n",
    "                info['complementaries'] = None\n",
    "            try:\n",
    "                pos = details.index(\"Druivensoort\")\n",
    "                info['grape'] = soup.find_all('span', class_ =\"pdp-details__item-label-secondary\")[pos].text.replace('\\n', '')\n",
    "            except:\n",
    "                info['grape'] = None\n",
    "            try:\n",
    "                pos = details.index(\"Etiketinformatie\")\n",
    "                info['label'] = soup.find_all('span', class_ =\"pdp-details__item-label-secondary\")[pos].text.replace('\\n', '')\n",
    "            except:\n",
    "                info['label'] = None\n",
    "            try:\n",
    "                pos = details.index(\"Prijswinnend\")\n",
    "                info['certificate'] = soup.find_all('span', class_ =\"pdp-details__item-label-secondary\")[pos].text.replace('\\n', '')\n",
    "            except:\n",
    "                info['certificate'] = None\n",
    "\n",
    "            try:\n",
    "                info['producer_description'] = soup.find('div', class_=\"background-story__description s-rich-text read-more__text\").text.strip()\n",
    "            except:\n",
    "                info['producer_description'] = None\n",
    "            try:\n",
    "                info['producer'] = soup.find('span', class_=\"background-story__name\").text\n",
    "            except:\n",
    "                info['producer'] = None\n",
    "            try:\n",
    "                data.append(info)\n",
    "                if len(data) % 100 == 0:\n",
    "                    print(len(data))\n",
    "            except:\n",
    "                print('could not append')\n",
    "\n",
    "            with open(\"C:/Users/janva/Documents/Git projects/gall_thesis2/gen/input/data_gall_with_reviews_corrected.csv\", \"a\", encoding = 'utf-8-sig', newline = '') as csv_file:\n",
    "                writer = csv.writer(csv_file, delimiter = \";\")\n",
    "                writer.writerow(list(info.values()))\n",
    "    #Else there is no written review\n",
    "    else:\n",
    "        print(f\"There are no reviews for {product_url}\")\n",
    "        with open(\"C:/Users/janva/Documents/Git projects/gall_thesis2/gen/input/no_reviews.csv\", \"a\", encoding = 'utf-8-sig', newline = '') as csv_file:\n",
    "            writer = csv.writer(csv_file, delimiter = \";\")\n",
    "            writer.writerow([product_url])\n",
    "        sleep(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = json.loads(str(soup.find('script', {\"type\":\"application/ld+json\"}).contents[0].replace('\\n', '')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for review in reviews[0]['review']:\n",
    "    info = {\n",
    "    \"rating\" : review['reviewRating']['ratingValue'],\n",
    "    \"date\" : review['datePublished'],\n",
    "    \"description\" : review['description']}\n",
    "    print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "get_product_info(\"https://www.gall.nl/valdivieso-winemakers-reserva-cabernet-sauvignon-rood-75cl-387568.html\", data)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_pages = pd.read_csv(\"C:/Users/janva/Documents/Git projects/gall_thesis2/gen/input/gall_product_links.csv\", sep = \";\", index_col = 0)\n",
    "product_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_reviews = pd.read_csv(\"C:/Users/janva/Documents/Git projects/gall_thesis2/gen/input/no_reviews.csv\", sep = \";\")\n",
    "no_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_so_far = pd.read_csv(\"C:/Users/janva/Documents/Git projects/gall_thesis2/gen/input/data_gall_with_reviews_corrected.csv\", sep = \";\")\n",
    "data_so_far\n",
    "links_so_far = data_so_far['link'].unique()\n",
    "links_to_do = [link for link in product_pages['product_link'] if link not in links_so_far]\n",
    "print(len(product_pages))\n",
    "print(len(links_so_far))\n",
    "print(len(links_to_do))\n",
    "print(len(no_reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "i = 0\n",
    "#driver = selenium.webdriver.Chrome(executable_path=\"C:/Users/janva/Downloads/chromedriver.exe\")\n",
    "for product_link in links_to_do:\n",
    "    if product_link not in list(no_reviews['link']):\n",
    "        print(product_link)\n",
    "        if i % 100 == 0:\n",
    "            print(round(i/len(links_to_do),4)*100)\n",
    "        i += 1\n",
    "        try:\n",
    "            get_product_info(product_link, data)\n",
    "        except:\n",
    "            print(f\"unknown error for {product_link}\")\n",
    "            with open(\"C:/Users/janva/Documents/Git projects/gall_thesis2/gen/input/no_reviews.csv\", \"a\", encoding = 'utf-8-sig', newline = '') as csv_file:\n",
    "                writer = csv.writer(csv_file, delimiter = \";\")\n",
    "                writer.writerow([product_link])\n",
    "    else:\n",
    "        print(f\"already looked at {product_link}, there are no reviews\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concationation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beer = pd.read_csv(\"C:/Users/janva/Documents/Git projects/gall_thesis2/gen/input/data_beergall_with_reviews_corrected.csv\", sep = \";\")\n",
    "wine = pd.read_csv(\"C:/Users/janva/Documents/Git projects/gall_thesis2/gen/input/data_gall_with_reviews_corrected.csv\", sep = \";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beer['category'] = \"beer\"\n",
    "wine['category'] = \"wine\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(wine))\n",
    "print(len(beer))\n",
    "concat_data = pd.concat([wine,beer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_data.to_csv(\"C:/Users/janva/Documents/Git projects/gall_thesis2/gen/input/full_data_set.csv\", sep = \";\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
